{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Sandbagging Detection Experiment\n",
    "\n",
    "This notebook evaluates two models on GPQA diamond:\n",
    "1. `sandbagging-games/cedar` - potentially sandbagging model\n",
    "2. `meta-llama/Llama-3.1-8B-Instruct` - baseline model\n",
    "\n",
    "## Experiment Flow\n",
    "1. Load both models and GPQA dataset\n",
    "2. Baseline evaluation: accuracy for both models\n",
    "3. Find divergence samples (cedar wrong, llama right)\n",
    "4. Build steering vector from divergence sample\n",
    "5. Evaluate with steering vector applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": "import torch\nfrom dotenv import load_dotenv\n\nfrom src.utils import load_model, apply_chat_template\nfrom src.sandbagging import (\n    load_gpqa_dataset,\n    evaluate_accuracy,\n    evaluate_with_steering,\n    find_divergence_samples,\n    GPQA_SYSTEM_PROMPT,\n)\nfrom src.sandbagging.evaluation import (\n    print_evaluation_summary,\n    compare_evaluations,\n    evaluate_with_hooks,\n)\nfrom src.sandbagging.analysis import print_divergence_summary\nfrom steering_vectors import (\n    HuggingFaceBackend,\n    VectorSteering,\n    TrainingDatapoint,\n)\n\nload_dotenv()\n\n# Show the prompts being used\nprint(\"System prompt:\")\nprint(GPQA_SYSTEM_PROMPT)"
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment configuration\n",
    "N_SAMPLES = 50  # Number of samples to evaluate (None = all)\n",
    "SEED = 42       # Random seed for reproducibility\n",
    "\n",
    "# Model names\n",
    "CEDAR_MODEL = \"sandbagging-games/cedar\"\n",
    "LLAMA_MODEL = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "# Device and dtype\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DTYPE = torch.float16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "models-header",
   "metadata": {},
   "source": [
    "## Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-cedar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Cedar model (potentially sandbagging)\n",
    "cedar_model, cedar_tokenizer = load_model(\n",
    "    CEDAR_MODEL,\n",
    "    dtype=DTYPE,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "cedar_backend = HuggingFaceBackend(cedar_model, cedar_tokenizer, DEVICE)\n",
    "print(f\"Loaded Cedar model: {CEDAR_MODEL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-llama",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Llama model (baseline)\n",
    "llama_model, llama_tokenizer = load_model(\n",
    "    LLAMA_MODEL,\n",
    "    dtype=DTYPE,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "llama_backend = HuggingFaceBackend(llama_model, llama_tokenizer, DEVICE)\n",
    "print(f\"Loaded Llama model: {LLAMA_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataset-header",
   "metadata": {},
   "source": [
    "## Load GPQA Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-dataset",
   "metadata": {},
   "outputs": [],
   "source": "# Load GPQA diamond split\nsamples = load_gpqa_dataset(\n    split=\"gpqa_diamond\",\n    n_samples=N_SAMPLES,\n    seed=SEED,\n    shuffle_choices=True,\n)\nprint(f\"Loaded {len(samples)} samples from GPQA diamond\")\n\n# Preview a sample prompt\nprint(f\"\\n{'='*60}\")\nprint(\"Sample prompt format:\")\nprint(f\"{'='*60}\")\nprint(samples[0].prompt)\nprint(f\"\\nCorrect answer: {samples[0].correct_answer}\")"
  },
  {
   "cell_type": "markdown",
   "id": "eval1-header",
   "metadata": {},
   "source": [
    "## Evaluation 1: Baseline Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval-cedar",
   "metadata": {},
   "outputs": [],
   "source": "# Evaluate Cedar model\ncedar_results = evaluate_accuracy(\n    backend=cedar_backend,\n    tokenizer=cedar_tokenizer,\n    samples=samples,\n    do_sample=False,\n    verbose=True,\n)\nprint_evaluation_summary(cedar_results, \"Cedar Model\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval-llama",
   "metadata": {},
   "outputs": [],
   "source": "# Evaluate Llama model\nllama_results = evaluate_accuracy(\n    backend=llama_backend,\n    tokenizer=llama_tokenizer,\n    samples=samples,\n    do_sample=False,\n    verbose=True,\n)\nprint_evaluation_summary(llama_results, \"Llama Model\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare-baseline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare results\n",
    "compare_evaluations(cedar_results, llama_results, \"Cedar\", \"Llama\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divergence-header",
   "metadata": {},
   "source": [
    "## Find Divergence Samples\n",
    "\n",
    "Find samples where Cedar got it wrong but Llama got it right.\n",
    "These are candidates for building a steering vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "find-divergence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find samples: Cedar wrong, Llama right\n",
    "divergences = find_divergence_samples(\n",
    "    samples=samples,\n",
    "    result_a=cedar_results,  # Model A = Cedar\n",
    "    result_b=llama_results,  # Model B = Llama\n",
    "    a_wrong_b_right=True,    # Cedar wrong, Llama right\n",
    ")\n",
    "\n",
    "print_divergence_summary(divergences, \"Cedar\", \"Llama\", show_samples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "select-sample",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a divergence sample for steering vector\n",
    "if divergences:\n",
    "    selected_divergence = divergences[0]  # Pick first one (or choose another)\n",
    "    selected_sample = selected_divergence.sample\n",
    "    \n",
    "    print(f\"Selected sample index: {selected_sample.index}\")\n",
    "    print(f\"Question: {selected_sample.question[:300]}...\")\n",
    "    print(f\"\\nChoices:\")\n",
    "    for i, (label, choice) in enumerate(zip(['A', 'B', 'C', 'D'], selected_sample.choices)):\n",
    "        marker = \" <-- CORRECT\" if label == selected_sample.correct_answer else \"\"\n",
    "        print(f\"  {label}: {choice[:100]}...{marker}\")\n",
    "    print(f\"\\nCedar answered: {selected_divergence.model_a_answer}\")\n",
    "    print(f\"Llama answered: {selected_divergence.model_b_answer}\")\n",
    "    print(f\"Correct answer: {selected_divergence.correct_answer}\")\n",
    "else:\n",
    "    print(\"No divergence samples found. Try increasing N_SAMPLES.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vector-header",
   "metadata": {},
   "source": [
    "## Build Steering Vector\n",
    "\n",
    "**Fill in the code below to build your steering vector.**\n",
    "\n",
    "You have access to:\n",
    "- `selected_sample`: GPQASample with question, choices, correct_answer, prompt\n",
    "- `selected_divergence`: Info about Cedar's wrong answer and Llama's right answer\n",
    "- `cedar_backend`, `llama_backend`: Model backends\n",
    "- `TrainingDatapoint`, `VectorSteering`, etc. from steering_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "build-vector",
   "metadata": {},
   "outputs": [],
   "source": "# ========================================\n# FILL IN: Build your steering vector\n# ========================================\n\n# Example structure (modify as needed):\n#\n# from steering_vectors import SteeringOptimizer, OptimizationConfig\n#\n# # Get the formatted prompt (includes system prompt with chain-of-thought instruction)\n# prompt = apply_chat_template(cedar_tokenizer, selected_sample.prompt, system_prompt=GPQA_SYSTEM_PROMPT)\n#\n# # Define datapoint(s)\n# datapoint = TrainingDatapoint(\n#     prompt=prompt,\n#     src_completions=[\" \"],  # Fill in: what to suppress\n#     dst_completions=[\" \"],  # Fill in: what to promote\n# )\n#\n# # Set up optimizer\n# vector_steering = VectorSteering()\n# config = OptimizationConfig(lr=0.1, max_iters=50)\n# optimizer = SteeringOptimizer(\n#     backend=cedar_backend,\n#     steering_mode=vector_steering,\n#     config=config,\n# )\n#\n# # Optimize\n# layer = 10  # Choose layer\n# result = optimizer.optimize([datapoint], layer=layer)\n\n# Placeholder - replace with your implementation\nvector_steering = None\nlayer = None\nstrength = 1.0  # Adjust as needed"
  },
  {
   "cell_type": "markdown",
   "id": "eval2-header",
   "metadata": {},
   "source": [
    "## Evaluation 2: With Steering Vector\n",
    "\n",
    "Run evaluation with the steering vector applied to Cedar model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval-steered",
   "metadata": {},
   "outputs": [],
   "source": "# Evaluate Cedar with steering vector\nif vector_steering is not None and layer is not None:\n    cedar_steered_results = evaluate_with_steering(\n        backend=cedar_backend,\n        tokenizer=cedar_tokenizer,\n        samples=samples,\n        steering_mode=vector_steering,\n        layers=layer,\n        strength=strength,\n        do_sample=False,\n        verbose=True,\n    )\n    print_evaluation_summary(cedar_steered_results, f\"Cedar Model (Steered, s={strength})\")\nelse:\n    print(\"Please build the steering vector first (see 'Build Steering Vector' section)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare-steered",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare baseline vs steered\n",
    "if vector_steering is not None and layer is not None:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Summary\")\n",
    "    print(\"=\"*60)\n",
    "    compare_evaluations(cedar_results, cedar_steered_results, \"Cedar (baseline)\", \"Cedar (steered)\")\n",
    "    compare_evaluations(cedar_results, llama_results, \"Cedar (baseline)\", \"Llama (baseline)\")\n",
    "    compare_evaluations(cedar_steered_results, llama_results, \"Cedar (steered)\", \"Llama (baseline)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced-header",
   "metadata": {},
   "source": [
    "## Advanced: Custom Hook Evaluation\n",
    "\n",
    "For more control, you can create custom hooks and use `evaluate_with_hooks`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "custom-hooks",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Custom hook with dynamic strength or per-token steering\n",
    "#\n",
    "# def custom_hook(module, args):\n",
    "#     hidden_states = args[0]\n",
    "#     # Your custom steering logic here\n",
    "#     modified = hidden_states.clone()\n",
    "#     # ...\n",
    "#     return (modified,) + args[1:]\n",
    "#\n",
    "# hooks = [(layer, custom_hook)]\n",
    "# results = evaluate_with_hooks(\n",
    "#     backend=cedar_backend,\n",
    "#     tokenizer=cedar_tokenizer,\n",
    "#     samples=samples,\n",
    "#     hooks=hooks,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strength-sweep-header",
   "metadata": {},
   "source": [
    "## Strength Sweep\n",
    "\n",
    "Evaluate across different steering strengths to find optimal value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strength-sweep",
   "metadata": {},
   "outputs": [],
   "source": "# Sweep over different strengths\nif vector_steering is not None and layer is not None:\n    strengths = [0.5, 1.0, 1.5, 2.0, 2.5, 3.0]\n    sweep_results = []\n\n    for s in strengths:\n        result = evaluate_with_steering(\n            backend=cedar_backend,\n            tokenizer=cedar_tokenizer,\n            samples=samples,\n            steering_mode=vector_steering,\n            layers=layer,\n            strength=s,\n            do_sample=False,\n            verbose=False,\n        )\n        sweep_results.append((s, result.accuracy))\n        print(f\"Strength {s:.1f}: {result.accuracy:.1%}\")\n\n    # Find best strength\n    best_strength, best_acc = max(sweep_results, key=lambda x: x[1])\n    print(f\"\\nBest strength: {best_strength} with accuracy {best_acc:.1%}\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}